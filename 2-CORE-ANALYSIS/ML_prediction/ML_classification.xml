<tool id="ML_prediction" name="Machine Learning-based Classification" version="18.09">
    <description></description>
    <command><![CDATA[
    #if $ifCV.cv == "true":
        #if $method.algorithm == "randomForest":
            /home/miniconda2/bin/ ${__tool_directory__}/03_mlClassification.py -positive ${posMat} -negative ${negMat} -m "randomForest" -estimators $method.ntree -t $method.cpus -out ${__tool_directory__}/  -k $ifCV.kfold;
        #elif $method.algorithm == "svm":
            #if $method.kernel.method == "polynomial":
                /home/miniconda2/bin/ ${__tool_directory__}/03_mlClassification.py -positive ${posMat} -negative ${negMat} -m "SVM" -kernel "poly" -degree $method.kernel.degree -coef0 $method.kernel.coef0 -gamma $method.kernel.gamma -out ${__tool_directory__}/  -k $ifCV.kfold;
            #elif $method.kernel.method == "radial_basis":
                /home/miniconda2/bin/ ${__tool_directory__}/03_mlClassification.py -positive ${posMat} -negative ${negMat} -m "SVM" -kernel "rbf" -gamma $method.kernel.gamma -out ${__tool_directory__}/  -k $ifCV.kfold;
            #elif $method.kernel.method == "sigmoid":
                /home/miniconda2/bin/ ${__tool_directory__}/03_mlClassification.py -positive ${posMat} -negative ${negMat} -m "SVM" -kernel "sigmoid" -gamma $method.kernel.gamma -out ${__tool_directory__}/  -k $ifCV.kfold;
            #else:
                /home/miniconda2/bin/ ${__tool_directory__}/03_mlClassification.py -positive ${posMat} -negative ${negMat} -m "SVM" -kernel "linear" -out ${__tool_directory__}/  -k $ifCV.kfold;
            #end if
        #elif $method.algorithm == "decisionTree":
            /home/miniconda2/bin/ ${__tool_directory__}/03_mlClassification.py -positive ${posMat} -negative ${negMat} -m "decisionTree" -out ${__tool_directory__}/ -k $ifCV.kfold;
        #elif $method.algorithm == "XGBoost":
            /home/miniconda2/bin/ ${__tool_directory__}/03_mlClassification.py -positive ${posMat} -negative ${negMat} -m "XGBoost" -lr $method.lr -estimators $method.ntree -t $method.cpus -out ${__tool_directory__}/ -k $ifCV.kfold;
        #else:
            /home/miniconda2/bin/ ${__tool_directory__}/03_mlClassification.py -positive ${posMat} -negative ${negMat} -m "LogisticRegression" -solver $method.solver -t $method.cpus -out ${__tool_directory__}/ -k $ifCV.kfold;
        #end if
        mv ${__tool_directory__}/model.pkl ${model};
        mv ${__tool_directory__}/CV_evaluation.pdf ${figure};
    #else:
        #if $method.algorithm == "randomForest":
            /home/miniconda2/bin/ ${__tool_directory__}/03_mlClassification.py -positive ${posMat} -negative ${negMat} -m "randomForest" -estimators $method.ntree -t $method.cpus -out ${__tool_directory__}/ -k 0;
        #elif $method.algorithm == "svm":
            #if $method.kernel.method == "polynomial":
                /home/miniconda2/bin/ ${__tool_directory__}/03_mlClassification.py -positive ${posMat} -negative ${negMat} -m "SVM" -kernel "poly" -degree $method.kernel.degree -coef0 $method.kernel.coef0 -gamma $method.kernel.gamma -out ${__tool_directory__}/  -k 0;
            #elif $method.kernel.method == "radial_basis":
                /home/miniconda2/bin/ ${__tool_directory__}/03_mlClassification.py -positive ${posMat} -negative ${negMat} -m "SVM" -kernel "rbf" -gamma $method.kernel.gamma -out ${__tool_directory__}/  -k 0;
            #elif $method.kernel.method == "sigmoid":
                /home/miniconda2/bin/ ${__tool_directory__}/03_mlClassification.py -positive ${posMat} -negative ${negMat} -m "SVM" -kernel "sigmoid" -gamma $method.kernel.gamma -out ${__tool_directory__}/  -k 0;
            #else:
                /home/miniconda2/bin/ ${__tool_directory__}/03_mlClassification.py -positive ${posMat} -negative ${negMat} -m "SVM" -kernel "linear" -out ${__tool_directory__}/  -k 0;
            #end if
        #elif $method.algorithm == "decisionTree":
            /home/miniconda2/bin/ ${__tool_directory__}/03_mlClassification.py -positive ${posMat} -negative ${negMat} -m "decisionTree" -out ${__tool_directory__}/ -k 0;
        #elif $method.algorithm == "XGBoost":
            /home/miniconda2/bin/ ${__tool_directory__}/03_mlClassification.py -positive ${posMat} -negative ${negMat} -m "XGBoost" -estimators $method.ntree -t $method.cpus -lr $method.lr -out ${__tool_directory__}/ -k 0;
        #else:
            /home/miniconda2/bin/ ${__tool_directory__}/03_mlClassification.py -positive ${posMat} -negative ${negMat} -m "LogisticRegression" -solver $method.solver -t $method.cpus -out ${__tool_directory__}/ -k 0;
        #end if
        mv ${__tool_directory__}/model.pkl ${model};
    #end if

    ]]></command>

    <inputs>
        <param name="posMat" type="data" format="txt" label="Feature matrix of positive samples."/>
        <param name="negMat" type="data" format="txt" label="Feature matrix of negative samples."/>
        <conditional name="method">
            <param name="algorithm" type="select" label="Select a machine learning algorithm"  display="radio">
                <option value="randomForest" selected="True">Random Forest</option>
                <option value="svm">Support Vector Machine</option>
                <option value="decisionTree">Decision tree</option>
                <option value="XGBoost">XGBoost</option>
                <option value="LogisticRegression">Logistic Regression</option>
            </param>
            <when value="randomForest">
                <param name="ntree" type="integer" value="500" label="Number of trees to grow." help="This should not be set to too small a number, to ensure that every input row gets predicted at least a few times."/>
                <param name="cpus" type="integer" value="1" min="1" max="10" label="The number of threads used for parallel computation."/>
            </when>
            <when value="svm">
                <conditional name="kernel">
                    <param name="method" type="select" label="Select a kernel method for SVM" display="radio">
                        <option value="polynomial" selected="True">Polynomial</option>
                        <option value="radial_basis">Radial_basis</option>
                        <option value="sigmoid">Sigmoid</option>
                        <option value="linear">Linear</option>
                    </param>
                    <when value="polynomial">
                        <param name="degree" type="integer" value="3" label="degree"/>
                        <param name="gamma" type="select" label="gamma" display="radio">
                            <option value="auto" selected="True">auto</option>
                            <option value="scale">scale</option>
                        </param>
                        <param name="coef0" type="float" value="3" label="coef0"/>
                    </when>
                    <when value="radial_basis">
                        <param name="gamma" type="select" label="gamma" display="radio">
                            <option value="auto" selected="True">auto</option>
                            <option value="scale">scale</option>
                        </param>
                    </when>
                    <when value="sigmoid">
                        <param name="gamma" type="select" label="gamma" display="radio">
                            <option value="auto" selected="True">auto</option>
                            <option value="scale">scale</option>
                        </param>
                    </when>
                </conditional>
            </when>
            <when value="XGBoost">
                <param name="lr" type="float" value="0.1" label="Learning rate"/>
                <param name="ntree" type="integer" value="500" label="Number of trees to grow." help="This should not be set to too small a number, to ensure that every input row gets predicted at least a few times."/>
                <param name="cpus" type="integer" value="1" min="1" max="10" label="The number of threads used for parallel computation."/>
            </when>
            <when value="LogisticRegression">
                <param name="solver" type="select" label="Select an optimization algorithm">
                        <option value="liblinear"  selected="True">liblinear</option>
                        <option value="newton-cg">newton-cg</option>
                        <option value="lbfgs">lbfgs</option>
                        <option value="sag">sag</option>
                        <option value="saga">saga</option>
                </param>
                <param name="cpus" type="integer" value="1" min="1" max="10" label="The number of threads used for parallel computation."/>
            </when>
	    </conditional>
        <conditional name="ifCV">
            <param name="cv" type="select" label="Select an evaluation strategy.">
                <option value="true">Cross validation evaluation</option>
                <option value="false" selected="True">Independent test evaluation</option>
            </param>
            <when value="true">
                <param name="kfold" type="integer" value="5" label="k-fold cross validation"/>
            </when>
        </conditional>
    </inputs>

    <outputs>
        <data format="pdf" name="figure" label="${method.algorithm}_CV_evaluation.pdf">
            <filter>ifCV['cv'] == "true"</filter>
        </data>
        <data name="model" label="${method.algorithm}_model.pkl">
		</data>
    </outputs>
    <help>
        .. class:: infomark

        **What it does**

        In this module, several commonly-used machine learning classification algorithms are implemented to construct a CMR predictor. In the current version of DeepEA, the following five classical algorithms are inlcuded:

        - Random Forest
        - Support Vector Machine
        - Decision Tree
        - XGBoost
        - Logistic Regression

        .. class:: infomark

        **Inputs**

        - **Feature matrix of positive samples**
        - **Feature matrix of negative samples**

        .. class:: infomark

        **Parameters**

        - For **Random Forest**
            - The number of trees
            - The number of threads used for parallel computation
        - For **Support Vector Machine**
            - Select a kernel method
        - For **XGBoost**
            - Learning rate
            - The number of trees
            - The number of threads used for parallel computation
        - For **Logistic Regression**
            - The optimization algorithm
            - The number of threads used for parallel computation

        .. class:: infomark

        **Outputs**

        - A CMR predictor in binary format
        - Cross validation evaluation results in PDF format
    </help>
    <citations>
        <citation type="bibtex">@article{pedregosa2011scikit,
                                title={Scikit-learn: Machine learning in Python},
                                author={Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},
                                journal={Journal of machine learning research},
                                volume={12},
                                number={Oct},
                                pages={2825--2830},
                                year={2011}
                                }
        </citation>
    </citations>
</tool>
